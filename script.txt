1. Data Preparation
--------------------

a. Move creditcard_defaulters_pst, creditcard_defaulters_cst and custmaster to /home/hduser/creditcard_insurance/
b. In mysql

	create database if not exists custdb;
	use custdb;
	drop table if exists credits_pst;
	drop table if exists credits_cst;
	drop table if exists custmaster;
	
	create table if not exists credits_pst (id integer,lmt integer,sex integer,edu integer,marital integer,age
	integer,pay integer,billamt integer,defaulter integer,issuerid1 integer,issuerid2 integer,tz varchar(3));

	create table if not exists credits_cst (id integer,lmt integer,sex integer,edu integer,marital integer,age
	integer,pay integer,billamt integer,defaulter integer,issuerid1 integer,issuerid2 integer,tz varchar(3));

	create table if not exists custmaster (id integer,fname varchar(100),lname varchar(100),ageval
	integer,profession varchar(100));

	source /home/hduser/creditcard_insurance/2_2_creditcard_defaulters_pst
	source /home/hduser/creditcard_insurance/2_creditcard_defaulters_cst
	source /home/hduser/creditcard_insurance/custmaster

c. In Linux sesison

    #S3 to HDFS
    bash sfm_insuredata.sh https://ashok-amsamani.s3.amazonaws.com/CCDefaulters/insuranceinfo.csv
    #It creates a file at /user/hduser/insurance_clouddata/creditcard_insurance_<today's date>
 
OBJECTIVE 1 - SPLIT DEFAULTERS AND NON-DEFAULTERS INTO 2 DATA SETS AND LOAD INTO HDFS.
======================================================================================
 
2. Sqoop integration with Hive (Merge cst and pst dataset in Hive)

sqoop import --connect jdbc:mysql://localhost/custdb \
--username root \
--password root \
--direct \
--m 4 \
--split-by "id" \
--query "select id,issuerid1,defaulter,billamt,pay,age,marital,edu,sex,lmt,tz from (
select id,issuerid1,defaulter,billamt,pay,age,marital,edu,sex,lmt,tz from credits_pst where billamt > 0
union all
select id,issuerid1,defaulter,billamt,pay,age,marital,edu,sex,lmt,tz from credits_cst where billamt > 0 ) as temp
WHERE \$CONDITIONS" \
--boundary-query "SELECT MIN(id), MAX(id) FROM (select distinct id from credits_pst where billamt > 0
union all
select distinct id from credits_cst where billamt > 0) as temp" \
--target-dir "/user/hduser/credits_pst_cst" \
--delete-target-dir \
--null-string "NA" \
--null-non-string "-0101" \
--fetch-size 2000 \
--hive-import \
--hive-table insure.cstpstreorder \
--hive-overwrite \
--map-column-hive id=bigint,billamt=float;


=> sqoop-hive import creates only managed table.

ALTER TABLE insure.cstpstreorder SET TBLPROPERTIES('EXTERNAL'='TRUE')


3. Apply some penalty to defaulters.

show create table insure.cstpstreorder and alter its DDL

CREATE EXTERNAL TABLE `insure.cstpstpenality`(
  'id' bigint,
  'issuerid1' int,
  'lmt' int,
  'newlmt' int,
  'sex' int,
  'edu' int,
  'marital' int,
  'pay' int,
  'billamt' float,
  'newbillamt' float,
  'defaulter' int)
STORED AS ORC
LOCATION '/user/hduser/cstpstreorder'

insert overwrite table insure.cstpstpenality 
select id,issuerid1,lmt,case defaulter when 1
then lmt-(lmt*0.04) else lmt end as newlmt ,sex,edu,marital,pay,billamt,case defaulter when
1 then billamt+(billamt*0.02) else billamt end as newbillamt, defaulter
from insure.cstpstreorder;

4. Data Provisioning using Hive, and DistCp (objective 1 is over here)

split the defaulters and non-defaulters into 2 data sets and load into hdfs. 

insert overwrite directory '/user/hduser/defaultersout/'
row format delimited fields terminated by ','
select * from insure.cstpstpenality where defaulter=1;

insert overwrite directory '/user/hduser/nondefaultersout/'
row format delimited fields terminated by ','
select concat(id, ', ', issuerid1, ', ', issuerid2, ', ', lmt, ', ', newlmt, ', ', sex, ', ', edu, ', ', marital,
',', pay, ', ', billamt, ', ', newbillamt, ', ', defaulter ) as cnt from insure.cstpstpenality
where defaulter=0
union
select concat('Trl|', count(1)) as cnt from insure.cstpstpenality where defaulter=0;


Below is to provision data to different region - sample.
hadoop distcp -overwrite hdfs://localhost:54310/user/hduser/nondefaultersout/ hdfs://localhost:54310/tmp/promocustomers

OBJECTIVE 2 - CONSOLIDATE ALL INFO IN SINGLE TABLE AND Find MAX PENALTIY FOR MALE AND FEMALE
=============================================================================================

5. Load S3 data to Hive as partition data.

CREATE TABLE insurance (IssuerId1 int,IssuerId2 int,BusinessYear int,StateCode string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan
string)
Partitioned by (datadt date,hr int)
row format delimited fields terminated by ','
TBLPROPERTIES ("skip.header.line.count"="1");

bash /home/hduser/creditcard_insurance/hivepart.sh /user/hduser/insurance_clouddata insure.insurance creditcard_insurance
# It creates static partition data in hive with date and hour from file.

#data cleansing on table using same table
insert overwrite table insurance partition(datadt,hr) select * from insurance where issuerid1 is not null;

6. Load fixed width state data to Hive

drop table if exists insure.state_master;

CREATE EXTERNAL TABLE insure.state_master (statecd STRING, statedesc STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe'
WITH SERDEPROPERTIES ("input.regex" = "(.{2})(.{20})" )
LOCATION '/user/hduser/states';
load data local inpath '/home/hduser/creditcard_insurance/states_fixedwidth' overwrite into table
insure.state_master; 

7. Create a table for Defaulters

use insure;
CREATE TABLE defaulters (id int,IssuerId1 int,IssuerId2 int,lmt int,newlmt double,sex int,edu int,marital
int,pay int,billamt int,newbillamt float,defaulter int)
row format delimited fields terminated by ','
LOCATION '/user/hduser/defaultersout'; 

8. Consolidate all data into staging and do cleansing, apply ORC, immutability and snappy for final table. Find max penalty for MALE and FEMALE. (Obejctive 2 ends here)

use insure;

CREATE TABLE insurancestg (IssuerId int,BusinessYear int,StateCode string,statedesc string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan
string,id int,lmt int,newlmt int,reduced_lmt int,sex varchar(6),grade varchar(20),marital int,pay
int,billamt int,newbillamt float,penality float,defaulter int)
row format delimited fields terminated by ','
LOCATION '/user/hduser/insurancestg';

insert overwrite table insurancestg select i.IssuerId1 as
issuerid,i.businessyear,i.statecode,s.statedesc as
statedesc,i.sourcename,i.networkname,i.networkurl,i.rownumber,i.marketcoverage,i.dentalonlyplan,
d.id,d.lmt,d.newlmt,d.newlmt-d.lmt as reduced_lmt,case when d.sex=1 then 'male' else 'female' end as
sex ,case when d.edu=1 then 'lower grade' when d.edu=2 then 'lower middle grade' when d.edu=3 then
'middle grade' when d.edu=4 then 'higher grade' when d.edu=5 then 'doctrate grade' end as grade
,d.marital,d.pay,d.billamt,d.newbillamt,d.newbillamt-d.billamt as penality,d.defaulter
from insurance i inner join defaulters d
on (i.IssuerId1=d.IssuerId1)
inner join state_master s
on (i.statecode=s.statecd);

dfs -rm -r -f /user/hduser/insuranceorc;
drop table if exists insuranceorc;

CREATE TABLE insuranceorc (IssuerId int,BusinessYear int,StateCode string,statedesc string,SourceName
string,NetworkName string,NetworkURL string,RowNumber int,MarketCoverage string,DentalOnlyPlan
string,id int,lmt int,newlmt int,reduced_lmt int,sex varchar(6),grade varchar(20),marital int,pay
int,billamt int,newbillamt float,penality int,defaulter int)
row format delimited fields terminated by ','
stored as orc
LOCATION '/user/hduser/insuranceorc'
TBLPROPERTIES ("immutable"="true","orc.compress"="SNAPPY");

Insert into insuranceorc select * from insurancestg where issuerid is not null;

alter table insuranceorc SET TBLPROPERTIES('EXTERNAL'='TRUE');

with T1 as ( select max(penality) as penalitymale from insuranceorc where sex='male'),
T2 as ( select max(penality) as penalityfemale from insuranceorc where sex='female')
select penalitymale,penalityfemale
from T1 inner join T2
ON 1=1;






